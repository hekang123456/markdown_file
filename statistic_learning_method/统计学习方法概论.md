# 1 统计学习
## 1.1 统计学习的特点
- 平台是计算机和网络
- 研究对象是数据
- 目的是对数据进行预测和分析
- 以方法为中心 （构建模型）
- 是概率论，统计学，信息论， 计算理论等多个领域的交叉学科

## 1.2 统计学习方法的三个要素
- 模型的假设空间（模型）
- 模型选择的准则（策略）
- 模型学习的算法（算法）

## 1.3 分类
- **监督学习**
- 非监督学习
- 半监督学习
- 强化学习

# 2 监督学习
## 2.1 基本概念
### 符号集合
| 符号 | 含义|
| :-: | :-: |
|$X$|输入变量， 文中的所有变量都是列向量|
|$Y$|输出变量， 文中的所有变量都是列向量|
|$x$|输入变量的取值 $x=(x^{(1)}, x^{(2)},...,x^{(i)},...,x^{(n)})^T$| 
|$x_i$| 第$i$个变量的取值 $x_i = (x_i^{(1)}, x_i^{(2)},...,x_i^{(i)},...,x_i^{(n)})^T$|
|$T$| 训练数据的集合 $T=\{(x_1,y_1), (x_2,y_2), ..., (x_N, y_N)\}$|
|$\mathcal{X}$|由输入变量 $X$ 组成的输入空间|
|$\mathcal{Y}$|由输出变量 $Y$ 组成的输入空间|
|$\mathcal{F}$|假设空间（模型空间，自己取得名字）|
|$L(Y, f(X))$| 由模型的预测$f(x)$ 和 实际标签$Y$ 组成的损失函数|

### 输入空间和输出空间
将输入和输出所有可能取值的集合称为输入空间。
### 特征向量（feature vector）
每个具体的输入是一个实例（instance），通常由特征向量表示。
### 特征空间（feature space）
所有的特征向量存在的空间称为特征空间。
### 回归问题和分类问题
- 回归问题： 输入变量和输出变量均为连续变量的预测问题。
- 分类问题： 输出变量为有限个离散变量的预测问题为分类问题。
### 联合概率分布和假设空间
- 监督学习的基本假设： 假设数据之间存在一定的统计规律， $X$ 和 $Y$之间存在着联合概率分布。 并且认为训练数据和测试数据是依联合概率分布 $P(X,Y)$ 独立同分布产生的。 
- 假设空间（hypothsis space）： 输入输出之间映射的集合（所有可能的模型）。 由条件概率 $P(Y|X)$ 或者决策函数（decision function) $Y=f(x)$ 表示。

## 3 统计学习中的三要素
统计学习方法由三部分组成可以简单的表示：
<center>方法 = 模型+策略+算法</center>
### 3.1 模型
在监督学习过程中模型就是所学的条件概率分布或者决策函数。  模型的假设空间$\mathcal{F}$由所有可能的条件概率分布或者决策函数组成。 因此假设空间$\mathcal{F}$可以通过下面这些式子表示。

- 决策函数的集合
  - 普通的写法
  $$ \mathcal{F} = \{ f| Y = f(X) \} $$
  - 由于假设空间通常是由一个参数向量决定的函数族，所以也可以写成
  $$ \mathcal{F} = \{ f| Y = f_\theta (X) \} $$
- 条件概率的集合
  - 一般的写法
  $$ \mathcal{F} = \{ P| P(Y|X) \} $$
  - 加上参数的写法
  $$ \mathcal{F} = \{ P| P_\theta (Y|X) \} $$
  
### 3.2 策略
tags: 损失函数， 风险函数， 期望风险， 期望损失， 经验风险， 经验损失，  结构风险
#### 3.2.1  [极大似然估计 + 最大后验概率估计](https://github.com/hekang123456/markdown_file/blob/master/statistic_learning_method/BT_MLE_MAP.md)
-  极大似然估计（maximum likelihood estimation 简称MLE）
  通过实验得到实验结果 $x_0$， 通过先验知识确定这个实验结果出现的概率函数称为似然函数 $P(x_0|\theta)$, 最后通过取对数值然后求导等方法求得使似然函数取最大值事的参数 $\theta$。
- 最大后验概率估计(maximum a posterior probability estimation 简称MAP)
  通过实验数据找到最合适的参数 $P(\theta|x_0)$, 由于 $P(\theta|x_0) = \frac{P(x_0|\theta) \times P(\theta)}{P(x_0)}$， 因为 $P(x_0)$ 可以通过实验得到，因此最大化后验只需要求最大化 $P(x_0|\theta) \times P(\theta)$ 前半部分是似然函数， 后半部分是先验。

#### 相关概念
- 损失函数（loss function）： 又称代价函数(cost function) 用来度量预测错误的程度。 损失函数是 $f(X)$ 与 $Y$ 的**非负实值函数**。 记为 $L(Y, f(X))$。 
  - 常见的损失函数
    - 0-1 损失函数
    $$
      \begin{align}
      y &= \sigma (W[x,y]+b)\\
      x &= 0
      \end{align}
    $$
  






