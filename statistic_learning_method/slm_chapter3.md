---
title: $k$ 近邻法
date: 2018-12-02
categories: [ML, 统计学习方法]
tags: [$k$近邻法]
---
# 1 $k$ 近邻法
- $k$ 近邻法的三个基本要素： $k$值的选择， 距离度量， 分类决策规则
## 1.1 $k$ 近邻算法
**输入**： 训练数据集
$$
T = \{ (x_1, y_1), (x_2, y_2),..., (x_N, y_N)\}
$$
其中， $x_i \in \mathcal{X}  \subseteq R^n$为实例的特征向量， $y_i \in  \mathcal{Y} ＝\{c_1， c_2,…,c_K\}$ 为实例的类别， $i＝1,2,…,N$； 实例特征向量$x$；
**输出**： 实例$x$所属的类$y$.
(1)  根据给定的距离度量， 在训练集$T$中找出与$x$最邻近的$k$个点， 涵盖这$k$个点的$x$的邻域记作$N_k(x)$；
(2) 在$N_k(x)$中根据分类决策规则（如多数表决） 决定$x$的类别$y$：
$$
y = \arg \max\limits_{c_j} \sum\limits_{x_i \in N_k (x)} I(y_i = c_j), i= 1,2,..,N; j=1,2,...,K
$$
式中$I$为指示函数，即当$y_i = c_j$时$I$为1， 否则为0。

(3) 最近邻算法：取 $k=1$ 时

## 1.2 距离度量
- $L_p$ 距离
    $$
    L_p (x_i, x_j) = \left( \sum\limits_{l=1}^n | x_i^{(l)} -x_j^{(l)} |^p \right)^{\frac{1}{p}}
    $$
	这里的$p\geq 1$,当$p​$取不同的值的时候会得到不同的距离。
	- 当 $p=2$时， 称为欧式距离（Euclidean distance）
	- 当 $p=1$时， 称为曼哈顿距离（Manhattan distance）
	- 当 $p=\infty$时， 取各个坐标距离的最大值，即 $L_{\infty} (x_i, x_j) = \max\limits_l |x_i^{(l)}-x_j^{(l)}|$

## 1.3 $k$ 值的选择
- 在应用中，$ k$值一般取一个比较小的数值。 通常采用交叉验证法来选取最优的$k$值.

## 1.4 分类决策规则
$k$ 近邻法的决策规则实际上是多数表决，如果分类函数的损失函数是0-1损失函数，将损失函数表示为：
$$f: R^n \rightarrow \{ c_1, c_2, ..., c_K \}$$
那么误分类的函数概率就是：
$$P(Y \neq f(X)) = 1-P(Y=f(X))$$
对给定的实例 $x \in X$, 其最近邻的$k$ 个训练实例点构成集合$N_k (x)$ 。如果涵盖$N_k(x)$ 的类别是$c_j$, 那么误分类率是:
$$\frac{1}{k} \sum\limits_{x_i \in N_k(x)} I(y_i \neq c_j)= 1- \frac{1}{k} \sum\limits_{x_i \in N_k(x)}I(y_i =c_j)$$ 
<font color=red>多数表决规则等价于经验风险最小化。</font>

## 1.5 $k$近邻法的实现： kd树
### 1.5.1 构造平衡kd树
**输入:** $k$ 维空间数据集 $T = \{ x_1, x_2, ..., x_N \}$, 其中$x_i = (x_i^{(1)}, x_i^{(2)}, x_i^{(3)}, ..., x_i^{(k)})^T$, $i = 1, 2, ..., N$
**输出：** kd树
(1) 开始：
- 构造根结点， 根结点对应于包含$T$的$k$维空间的超矩形区域。 选择$x^{(1)}$为坐标轴， 以$T$中所有实例的$x^{(1)}$坐标的中位数为切分点， 将根结点对应的超矩形区域切分为两个子区域。 切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。
- 由根结点生成深度为1的左、右子结点： 左子结点对应坐标$x^{(1)}$小于切分点的子区域， 右子结点对应于坐标$x^{(1)}$大于切分点的子区域。
- 将落在切分超平面上的实例点保存在根结点。

(2) 重复：
- 对深度为$j$的结点， 选择$x^{(l)}$为切分的坐标轴， $l＝j( \text{mod} k)+1$， 以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点， 将该结点对应的超矩形区域切分为两个子区域。 切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。
- 由该结点生成深度为$j+1$的左、 右子结点： 左子结点对应坐标$x^{(l)}$小于切分点的子区
域， 右子结点对应坐标$x^{(l)}$大于切分点的子区域。
- 直到两个子区域没有实例存在时停止。 从而形成kd树的区域划分。

### 1.5.2 搜索kd树
**输入：**  已构造的kd树； 目标点$x$；
**输出：** $x$的最近邻。
- 在kd树中找出包含目标点$x$的叶结点： 从根结点出发， 递归地向下访问kd树。 若目标点$x$当前维的坐标小于切分点的坐标， 则移动到左子结点， 否则移动到右子结点。 直到子结点为叶结点为止。
- 以此叶结点为“当前最近点”。
- 递归地向上回退， 在每个结点进行以下操作：
  - a. 如果该结点保存的实例点比当前最近点距离目标点更近， 则以该实例点为“当前最近点”。
  - b.
  	- 当前最近点一定存在于该结点一个子结点对应的区域。 检查该子结点的父结点的另一子结点对应的区域是否有更近的点。 具体地， 检查另一子结点对应的区域是否与以目标点为球心、 以目标点与“当前最近点”间的距离为半径的超球体相交。
  	- 如果相交， 可能在另一个子结点对应的区域内存在距目标点更近的点， 移动到另一个子结点。 接着， 递归地进行最近邻搜索
  	- 如果不相交， 向上回退。
- 当回退到根结点时， 搜索结束。 最后的“当前最近点”即为$x$的最近邻点。 如果实例点是随机分布的， kd树搜索的平均计算复杂度是$O(\log N)$， 这里$N$是训练实例数。 kd树更适用于训练实例数远大于空间维数时的$k$近邻搜索。 当空间维数接近训练实例数时， 它的效率会迅速下降， 几乎接近线性扫描
