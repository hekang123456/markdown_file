---
title: 在机器学习中数据不平衡问题的解决
categories: others
tags: [机器学习,  数据不平衡, SMOT, Border-line SMOT, ADASYN, 采样， 加权， 一分类]
date: 2018-11-25
---

# [在机器学习中数据不平衡问题的解决](https://www.cnblogs.com/zhaokui/p/5101301.html)
> 出处： https://www.cnblogs.com/zhaokui/p/5101301.html
###  例子：
在所有的微博对应的评论数量划分为1到5这5个等级。 1少5多。 大部分的评论都很少，极少数的微博评论会非常的多。 如果我们要对一些微博的评论数量进行预测。 只要全部预测为1，就能够得到非常高的准确率， 显然这样的预测是没有意义的。

### 将问题分为4个类别
| 数据量 | 分布是否均匀 |
| :-: | :-: |
| 大 | 均匀 |
| 大 | 不均匀 |
| 小 | 均匀 |
| 小 | 不均匀 |

- 注： 当每个类别的数据量大于5000 个以上的时候，正负样本数量相差一个量级是能够接受的（经验之谈）。 

### 主要的解决方法
- 采样

| 方法 | 做法 | 问题 | 解决方法 |
| - | :- |:- | :- |
|上采样| 小样本复制多分|过拟合|加入随机扰动|
|下采样|剔除一部分大样本|信息损失|**EasyEnsemble)**：多次放回的独立采样构建多个独立的模型，然后将多个模型进行组合 <br>**BalanceCascade**: 在EasyEnsemble的基础上前面训练得到的模型预测准确的样本不放回。<br>**NearMiss**: 利用KNN挑选出最具代表性的大众样本|

- 数据合成
	- SMOT
		- 方法： 对于小众样本 $x_i \in S_{min}$ 从它的 k 近邻中随机选取一个点$\hat{x}$。 生成新的小众样本 $x_{new}=x_i + (\hat{x}-x_i) \times \delta$ , 其中 $\delta \in [0,1]$ 是一个随机数。
		- 存在的问题： 1. 增加了类之间重叠的可能性。 2. 生成了一些没有提供有益信息的样本。 下面两种方法用来解决这些问题。
	- Borderline-SMOTE
只对小众样本中那些 k 近邻中大部分是大众样本的点通过SMOTE生成新样本。 因为这些样本往往是边界样本
	- ADASYN
		- 首先计算每个小众样本在需要使整个数据集达到平衡时需要增加的数据量记为 $G$.
		- 再计算对于具体的一个小众样本中每个点需要生成的样本占 $G$ 的比例。 
			$$\mathcal{T}_i = \frac{\Delta_{ik}}{\sum_i \Delta_{ik}} $$ 
			其中的$\Delta_{ik}$ 是第$i$ 个样本点中$k$近邻中大众样本的个数。
		- 计算小众样本中每个点需要利用SMOT方法生成的点的个数： $g_i = \mathcal{T}_i  \times G$

- 加权
	对于不同的类别分成其他的类别时对应的损失是不同的。 将 $c(i,j)$ 视为是把真实样本类别为 $j$ 的时候分类成 $i$ 时的损失。 该方法的难点在于如何确定 $c(i,j)$.
	
- 一分类
	当正负样本分布及其不均匀的时候，可以将这个模型看成是一分类或者异常检测的问题。 其中经典的工作包括 One-class SVM 。

### 方法的选择
	正负样本均很少： 数据合成
	正负样本比例悬殊：-分类
	数据量还行，比例不是特别悬殊： 采样和加权的方法
	
	采样和加权在数学上等价，但是实际中在计算资源合适的情况下，采样会好一点。
> 有空可以看看 Learning from Imbalanced Data 这篇综述。
